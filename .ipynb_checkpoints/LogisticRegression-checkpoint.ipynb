{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "607d1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d468837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression2():\n",
    "    def __init__(self,learning_rate=0.0001,n_iterations=400,threshold=.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self,X_train,y_train):\n",
    "        m ,n = X_train.shape\n",
    "        ones = np.ones((m,1))\n",
    "        X_new = np.concatenate((ones,X),axis=1)\n",
    "        self.weights = np.random.randn(n+1,1)\n",
    "        for i in range(self.n_iterations):\n",
    "            dw = (1/m)*X_new.T.dot(self.sigmoid(X_new.dot(self.weights)) - y_train)\n",
    "            self.weights = self.weights - self.learning_rate * dw\n",
    "        \n",
    "    def predict(self,X):\n",
    "        m = X.shape[0]\n",
    "        ones = np.ones((m,1))\n",
    "        X_new = np.concatenate((ones,X),axis=1)\n",
    "        return self.sigmoid(np.dot(X_new,self.weights)) > self.threshold\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 - np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4d5bc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression()\n",
    "log = LogisticRegression2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ab8b18f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'sigmoid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m Y \u001b[38;5;241m=\u001b[39m(\u001b[43mlog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'sigmoid'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "582dc79f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'weights'"
     ]
    }
   ],
   "source": [
    "log.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8cc2bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "log2.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "24db4175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oussa\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9a8ed83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.predict(np.array([[-1.19227158]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4e82e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2b60ef9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17506653]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8f22cb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29714078])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dddc6504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58852189],\n",
       "       [-1.66094064]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "083c4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, learning_rate: float = 0.001, standardize: bool = True,\n",
    "                 fit_intercept: bool = True, decision_threshold: float = 0.5,\n",
    "                 iterations: int = 500):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.standardize = standardize\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.decision_threshold = decision_threshold\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def prepare_features(self, X):\n",
    "        if self.standardize:\n",
    "            X = (X - np.mean(X, 0)) / np.std(X, 0)\n",
    "        if self.fit_intercept:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.append(bias, X, axis=1)\n",
    "        return X\n",
    "\n",
    "    def init_weight_matrix(self, num_features):\n",
    "        return np.zeros((num_features, 1))  # Shape: (m, 1)\n",
    "\n",
    "    def calculate_z(self, features, W):\n",
    "        return np.dot(features, W)  # shape(m, 1)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))  # shape: (m, 1)\n",
    "\n",
    "    def predict(self, y_prob):\n",
    "        return (y_prob > self.decision_threshold).astype(np.float64)  # shape: (m, 1)\n",
    "\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        return np.sum(-y * np.log(y_pred + epsilon) - (1 - y) * np.log(1 - y_pred + epsilon)) / len(y)  # scalar\n",
    "\n",
    "    def compute_gradients(self, features, y, y_pred):\n",
    "        return np.dot(features.T, (y_pred - y)) / len(y)  # shape: (n, 1)\n",
    "\n",
    "    def update_weights(self, W, gradients):\n",
    "        return W - self.learning_rate * gradients  # shape: (n, 1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = y.reshape((len(y), 1))\n",
    "        features = self.prepare_features(X)\n",
    "        num_samples, num_features = features.shape\n",
    "        W = self.init_weight_matrix(num_features)\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            z = self.calculate_z(features, W)\n",
    "            y_prob = self.sigmoid(z)\n",
    "            y_pred = self.predict(y_prob)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            print(f\"Iteration: {i} - Log Loss: {loss}\")\n",
    "            gradients = self.compute_gradients(features, y, y_pred)\n",
    "            W = self.update_weights(W, gradients)\n",
    "\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "422be4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - Log Loss: 5.802514370344998\n",
      "Iteration: 1 - Log Loss: 7.736685860459995\n",
      "Iteration: 2 - Log Loss: 0.3223618150191714\n",
      "Iteration: 3 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 4 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 5 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 6 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 7 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 8 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 9 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 10 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 11 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 12 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 13 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 14 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 15 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 16 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 17 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 18 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 19 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 20 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 21 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 22 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 23 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 24 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 25 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 26 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 27 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 28 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 29 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 30 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 31 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 32 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 33 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 34 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 35 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 36 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 37 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 38 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 39 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 40 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 41 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 42 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 43 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 44 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 45 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 46 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 47 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 48 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 49 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 50 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 51 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 52 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 53 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 54 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 55 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 56 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 57 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 58 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 59 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 60 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 61 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 62 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 63 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 64 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 65 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 66 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 67 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 68 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 69 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 70 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 71 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 72 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 73 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 74 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 75 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 76 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 77 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 78 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 79 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 80 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 81 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 82 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 83 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 84 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 85 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 86 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 87 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 88 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 89 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 90 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 91 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 92 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 93 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 94 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 95 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 96 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 97 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 98 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 99 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 100 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 101 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 102 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 103 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 104 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 105 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 106 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 107 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 108 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 109 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 110 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 111 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 112 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 113 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 114 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 115 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 116 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 117 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 118 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 119 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 120 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 121 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 122 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 123 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 124 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 125 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 126 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 127 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 128 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 129 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 130 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 131 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 132 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 133 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 134 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 135 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 136 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 137 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 138 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 139 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 140 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 141 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 142 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 143 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 144 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 145 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 146 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 147 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 148 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 149 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 150 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 151 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 152 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 153 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 154 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 155 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 156 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 157 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 158 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 159 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 160 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 161 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 162 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 163 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 164 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 165 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 166 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 167 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 168 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 169 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 170 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 171 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 172 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 173 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 174 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 175 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 176 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 177 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 178 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 179 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 180 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 181 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 182 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 183 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 184 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 185 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 186 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 187 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 188 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 189 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 190 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 191 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 192 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 193 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 194 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 195 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 196 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 197 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 198 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 199 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 200 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 201 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 202 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 203 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 204 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 205 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 206 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 207 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 208 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 209 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 210 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 211 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 212 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 213 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 214 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 215 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 216 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 217 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 218 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 219 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 220 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 221 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 222 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 223 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 224 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 225 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 226 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 227 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 228 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 229 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 230 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 231 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 232 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 233 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 234 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 235 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 236 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 237 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 238 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 239 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 240 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 241 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 242 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 243 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 244 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 245 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 246 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 247 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 248 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 249 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 250 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 251 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 252 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 253 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 254 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 255 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 256 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 257 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 258 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 259 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 260 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 261 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 262 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 263 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 264 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 265 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 266 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 267 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 268 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 269 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 270 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 271 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 272 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 273 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 274 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 275 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 276 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 277 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 278 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 279 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 280 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 281 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 282 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 283 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 284 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 285 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 286 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 287 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 288 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 289 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 290 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 291 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 292 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 293 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 294 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 295 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 296 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 297 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 298 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 299 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 300 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 301 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 302 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 303 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 304 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 305 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 306 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 307 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 308 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 309 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 310 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 311 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 312 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 313 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 314 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 315 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 316 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 317 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 318 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 319 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 320 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 321 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 322 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 323 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 324 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 325 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 326 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 327 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 328 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 329 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 330 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 331 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 332 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 333 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 334 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 335 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 336 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 337 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 338 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 339 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 340 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 341 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 342 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 343 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 344 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 345 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 346 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 347 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 348 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 349 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 350 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 351 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 352 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 353 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 354 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 355 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 356 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 357 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 358 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 359 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 360 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 361 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 362 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 363 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 364 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 365 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 366 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 367 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 368 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 369 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 370 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 371 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 372 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 373 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 374 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 375 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 376 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 377 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 378 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 379 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 380 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 381 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 382 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 383 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 384 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 385 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 386 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 387 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 388 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 389 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 390 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 391 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 392 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 393 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 394 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 395 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 396 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 397 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 398 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 399 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 400 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 401 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 402 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 403 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 404 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 405 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 406 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 407 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 408 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 409 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 410 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 411 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 412 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 413 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 414 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 415 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 416 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 417 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 418 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 419 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 420 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 421 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 422 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 423 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 424 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 425 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 426 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 427 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 428 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 429 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 430 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 431 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 432 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 433 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 434 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 435 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 436 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 437 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 438 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 439 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 440 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 441 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 442 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 443 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 444 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 445 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 446 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 447 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 448 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 449 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 450 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 451 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 452 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 453 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 454 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 455 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 456 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 457 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 458 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 459 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 460 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 461 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 462 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 463 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 464 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 465 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 466 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 467 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 468 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 469 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 470 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 471 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 472 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 473 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 474 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 475 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 476 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 477 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 478 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 479 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 480 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 481 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 482 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 483 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 484 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 485 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 486 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 487 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 488 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 489 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 490 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 491 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 492 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 493 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 494 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 495 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 496 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 497 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 498 - Log Loss: -9.999999505838703e-08\n",
      "Iteration: 499 - Log Loss: -9.999999505838703e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00014   ],\n",
       "       [-0.00052743]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "da8089f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.15779598],\n",
       "       [0.56473025]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8db5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150ab9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
